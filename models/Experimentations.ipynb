{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Experimentations.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zs1wJAhhhFGh",
        "outputId": "5b799cef-bc36-4213-a579-361efc98aa94"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54QVpmsiXzSx"
      },
      "source": [
        "import numpy as np\n",
        "import scipy\n",
        "import time\n",
        "from PIL import Image,ImageOps\n",
        "from keras.datasets import mnist\n",
        "from keras import layers\n",
        "import keras\n",
        "from keras.layers import Dropout\n",
        "from matplotlib import pyplot as plt\n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.utils import to_categorical\n",
        "from keras import initializers\n",
        "from keras import optimizers\n",
        "from matplotlib import pyplot\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.datasets import cifar10\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from array import array\n",
        "import cv2\n",
        "import os\n",
        "import random\n",
        "from google.colab.patches import cv2_imshow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehdV6ZvEuS4k"
      },
      "source": [
        "new_shape = (28,28)\n",
        "def load_images_from_folder(folder,images,y,i):\n",
        "  \n",
        "    for filename in os.listdir(folder):\n",
        "        img=cv2.imread(os.path.join(folder,filename))\n",
        "        #img=cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        img=cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "        #img = Image.open(os.path.join(folder,filename))\n",
        "        #print(type(img))\n",
        "        if img is not None:\n",
        "            #print(img.shape)\n",
        "            #plt.imshow(img)\n",
        "            img = cv2.resize(img,new_shape)\n",
        "            images.append(img) \n",
        "            \n",
        "            im=img.copy()\n",
        "            mean=0\n",
        "            st=0.7\n",
        "            gauss = np.random.normal(mean,st,im.shape)\n",
        "            gauss = gauss.astype('uint8')\n",
        "            im = cv2.add(im,gauss)\n",
        "            images.append(im)\n",
        "\n",
        "            # ima=img.copy()\n",
        "            # value = np.random.choice(np.array([-50, -40, -30, 30, 40, 50]))\n",
        "            # hsv = cv2.cvtColor(ima, cv2.COLOR_BGR2HSV)\n",
        "            # h, s, v = cv2.split(hsv)\n",
        "            # if value >= 0:\n",
        "            #   lim = 255 - value\n",
        "            #   v[v > lim] = 255\n",
        "            #   v[v <= lim] += value\n",
        "            # else:\n",
        "            #   lim = np.absolute(value)\n",
        "            #   v[v < lim] = 0\n",
        "            #   v[v >= lim] -= np.absolute(value)\n",
        "\n",
        "            # final_hsv = cv2.merge((h, s, v))\n",
        "            # ima = cv2.cvtColor(final_hsv, cv2.COLOR_HSV2BGR)\n",
        "          \n",
        "\n",
        "            # value = np.random.choice(np.array([-50, -40, -30, 30, 40, 50]))\n",
        "            # hsv = cv2.cvtColor(ima, cv2.COLOR_BGR2HSV)\n",
        "            # h, s, v = cv2.split(hsv)\n",
        "            # if value >= 0:\n",
        "            #   lim = 255 - value\n",
        "            #   s[s > lim] = 255\n",
        "            #   s[s <= lim] += value\n",
        "            # else:\n",
        "            #   lim = np.absolute(value)\n",
        "            #   s[s < lim] = 0\n",
        "            #   s[s >= lim] -= np.absolute(value)\n",
        "\n",
        "            # final_hsv = cv2.merge((h, s, v))\n",
        "            # ima = cv2.cvtColor(final_hsv, cv2.COLOR_HSV2BGR)\n",
        "\n",
        "            #images.append(ima)\n",
        "\n",
        "            im=img.copy()\n",
        "            brightness = 10\n",
        "            contrast = random.randint(40, 100)\n",
        "            dummy = np.int16(im)\n",
        "            dummy = dummy * (contrast/127+1) - contrast + brightness\n",
        "            dummy = np.clip(dummy, 0, 255)\n",
        "            im = np.uint8(dummy)\n",
        "            images.append(im)\n",
        "            # y.append(i-1)\n",
        "            y.append(i-1)\n",
        "            y.append(i-1)\n",
        "            y.append(i-1)\n",
        "            #i=i+1\n",
        "    #print(i)\n",
        "    return images,y\n",
        "\n",
        "X = []\n",
        "y=[]\n",
        "for i in range(1,11):\n",
        "  #print(i)\n",
        "  if (i<10):\n",
        "    folder=\"/content/gdrive/MyDrive/trainPart1/train/Sample00\"+str(i)\n",
        "  else:\n",
        "    folder=\"/content/gdrive/MyDrive/trainPart1/train/Sample0\"+str(i)\n",
        "  X,y=load_images_from_folder(folder,X,y,i)\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOwSlW-FYVDO",
        "outputId": "1e2fd695-1467-49ba-9bac-097be6ce3340"
      },
      "source": [
        "X=np.array(X)\n",
        "print(X.shape)\n",
        "y=np.array(y)\n",
        "print(y.shape)\n",
        "train_images, test_imagesall, train_labels, test_labelsall = train_test_split(X, y, test_size=0.20, random_state=42)\n",
        "\n",
        "print('Shape of images is..',train_images.shape)\n",
        "print('Shape of labels is..',train_labels.shape)\n",
        "\n",
        "train_images=train_images.reshape(960,28,28,1)\n",
        "test_imagesall=test_imagesall.reshape(240,28,28,1)\n",
        "\n",
        "val_images = test_imagesall[0:100,:,:]\n",
        "val_labels = test_labelsall[0:100]\n",
        "test_images = test_imagesall[100:,:,:]\n",
        "test_labels = test_labelsall[100:]\n",
        "\n",
        "\n",
        "# display sample\n",
        "print(\"Shape of training images:\",train_images.shape)\n",
        "print(\"Shape of training labels:\",train_labels.shape)\n",
        "print(\"unique labels:\",np.unique(train_labels))\n",
        "print(\"Shape of val images:\",val_images.shape)\n",
        "print(\"Shape of val labels:\",val_labels.shape)\n",
        "print(\"Shape of test images:\",test_images.shape)\n",
        "print(\"Shape of test labels:\",test_labels.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1200, 28, 28)\n",
            "(1200,)\n",
            "Shape of images is.. (960, 28, 28)\n",
            "Shape of labels is.. (960,)\n",
            "Shape of training images: (960, 28, 28, 1)\n",
            "Shape of training labels: (960,)\n",
            "unique labels: [0 1 2 3 4 5 6 7 8 9]\n",
            "Shape of val images: (100, 28, 28, 1)\n",
            "Shape of val labels: (100,)\n",
            "Shape of test images: (140, 28, 28, 1)\n",
            "Shape of test labels: (140,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUg9ANjlYVFe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba9f0ddc-6fc8-4d17-cf29-1dae33e604ac"
      },
      "source": [
        "train_images = (train_images / 255) - 0.5\n",
        "test_images = (test_images / 255) - 0.5\n",
        "val_images = (val_images / 255) - 0.5\n",
        "print(train_images.shape)\n",
        "# Flatten the images.\n",
        "\n",
        "# train_images = train_images.reshape((-1, 784))\n",
        "# test_images = test_images.reshape((-1, 784))\n",
        "# val_images = val_images.reshape((-1, 784))\n",
        "\n",
        "#train_images = train_images.reshape((-1, 784,3))\n",
        "\n",
        "#test_images = test_images.reshape((-1, 784,3))\n",
        "#val_images = val_images.reshape((-1, 784,3))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(960, 28, 28, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdknuuOpYtE4"
      },
      "source": [
        "# Build the model.\n",
        "# model = Sequential([\n",
        "#  Dense(64, activation='relu', input_shape=(784,)),\n",
        "#  Dense(64, activation='relu'),\n",
        "#  Dense(10, activation='softmax'),\n",
        "# ])\n",
        "\n",
        "# Build the model\n",
        "# model = Sequential() # create model\n",
        "# model.add(Dense(64, input_dim=784, trainable=True,activation='relu', use_bias=True, \n",
        "#                 kernel_initializer=initializers.he_normal(seed=None))) # hidden \n",
        "# model.add(Dropout(0.5))\n",
        "\n",
        "\n",
        "\n",
        "# # model.add(Dense(64, trainable=True, activation='relu', use_bias=True, \n",
        "# #                 kernel_initializer=initializers.he_normal(seed=None))) # hidden layer\n",
        "# # model.add(Dropout(0.5))\n",
        "\n",
        "# # model.add(Dense(128, use_bias=False))\n",
        "# # model.add(BatchNormalization())\n",
        "# # model.add(Activation(\"relu\"))\n",
        "\n",
        "# # model.add(Dense(128, use_bias=False))\n",
        "# # model.add(BatchNormalization())\n",
        "# # model.add(Activation(\"relu\"))\n",
        "\n",
        "# model.add(Dense(64, use_bias=False))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Activation(\"relu\"))\n",
        "\n",
        "# model.add(Dense(10, trainable=True, activation='softmax')) # output layer\n",
        "##################\n",
        "model = Sequential()\n",
        "\n",
        "model.add(layers.Conv2D(filters=6, kernel_size=(3, 3), activation='relu', input_shape=(28,28,1)))\n",
        "model.add(layers.AveragePooling2D())\n",
        "\n",
        "model.add(layers.Conv2D(filters=16, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(layers.AveragePooling2D())\n",
        "\n",
        "model.add(layers.Flatten())\n",
        "\n",
        "model.add(layers.Dense(units=120, activation='relu'))\n",
        "\n",
        "model.add(layers.Dense(units=84, activation='relu'))\n",
        "\n",
        "model.add(layers.Dense(units=10, activation = 'softmax'))\n",
        "\n",
        "# model=keras.applications.VGG19(\n",
        "#     include_top=False,\n",
        "#     weights=None,\n",
        "#     input_tensor=None,\n",
        "#     input_shape=(200,200,3),\n",
        "#     pooling=None,\n",
        "#     classes=62,\n",
        "#     classifier_activation=\"softmax\",\n",
        "# )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXF5ldQYZQDO",
        "outputId": "f724e885-f82d-4daa-870f-2cb74660048e"
      },
      "source": [
        "adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
        "\n",
        "model.compile(\n",
        "  optimizer=adam,\n",
        "  loss='categorical_crossentropy',\n",
        "  metrics=['accuracy'],\n",
        ")\n",
        "\n",
        "# obtain the summary\n",
        "#saved_model.summary()\n",
        "\n",
        "\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience = 5)\n",
        "mc = ModelCheckpoint('/content/gdrive/MyDrive/models/task2_pretrain.h5', monitor='val_accuracy', mode='max', \n",
        "                     verbose=1, save_best_only=True)\n",
        "print(train_images.shape)\n",
        "print(val_images.shape)\n",
        "# Train the model.\n",
        "history=model.fit(\n",
        "  train_images,\n",
        "  to_categorical(train_labels),\n",
        "  validation_data=(val_images, to_categorical(val_labels)),  \n",
        "  epochs=20,\n",
        "  batch_size=32,\n",
        "  shuffle = True,\n",
        "  callbacks=[es,mc],\n",
        ")\n",
        "\n",
        "# Evaluate the model.\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(960, 28, 28, 1)\n",
            "(100, 28, 28, 1)\n",
            "Epoch 1/20\n",
            "30/30 [==============================] - 33s 19ms/step - loss: 2.3043 - accuracy: 0.1025 - val_loss: 2.2400 - val_accuracy: 0.1800\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.18000, saving model to /content/gdrive/MyDrive/models/task2_pretrain.h5\n",
            "Epoch 2/20\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 2.2174 - accuracy: 0.2212 - val_loss: 2.1090 - val_accuracy: 0.2700\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.18000 to 0.27000, saving model to /content/gdrive/MyDrive/models/task2_pretrain.h5\n",
            "Epoch 3/20\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 2.0037 - accuracy: 0.3041 - val_loss: 1.8946 - val_accuracy: 0.4100\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.27000 to 0.41000, saving model to /content/gdrive/MyDrive/models/task2_pretrain.h5\n",
            "Epoch 4/20\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 1.6781 - accuracy: 0.4638 - val_loss: 1.6339 - val_accuracy: 0.4700\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.41000 to 0.47000, saving model to /content/gdrive/MyDrive/models/task2_pretrain.h5\n",
            "Epoch 5/20\n",
            "30/30 [==============================] - 0s 5ms/step - loss: 1.4857 - accuracy: 0.5320 - val_loss: 1.4118 - val_accuracy: 0.5100\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.47000 to 0.51000, saving model to /content/gdrive/MyDrive/models/task2_pretrain.h5\n",
            "Epoch 6/20\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 1.3038 - accuracy: 0.5671 - val_loss: 1.3663 - val_accuracy: 0.5500\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.51000 to 0.55000, saving model to /content/gdrive/MyDrive/models/task2_pretrain.h5\n",
            "Epoch 7/20\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 1.1607 - accuracy: 0.6001 - val_loss: 1.2551 - val_accuracy: 0.5900\n",
            "\n",
            "Epoch 00007: val_accuracy improved from 0.55000 to 0.59000, saving model to /content/gdrive/MyDrive/models/task2_pretrain.h5\n",
            "Epoch 8/20\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 1.0220 - accuracy: 0.6642 - val_loss: 1.1560 - val_accuracy: 0.6400\n",
            "\n",
            "Epoch 00008: val_accuracy improved from 0.59000 to 0.64000, saving model to /content/gdrive/MyDrive/models/task2_pretrain.h5\n",
            "Epoch 9/20\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.9921 - accuracy: 0.6736 - val_loss: 1.0779 - val_accuracy: 0.6600\n",
            "\n",
            "Epoch 00009: val_accuracy improved from 0.64000 to 0.66000, saving model to /content/gdrive/MyDrive/models/task2_pretrain.h5\n",
            "Epoch 10/20\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.8518 - accuracy: 0.7179 - val_loss: 1.0494 - val_accuracy: 0.6500\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.66000\n",
            "Epoch 11/20\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.8077 - accuracy: 0.7361 - val_loss: 1.0641 - val_accuracy: 0.6700\n",
            "\n",
            "Epoch 00011: val_accuracy improved from 0.66000 to 0.67000, saving model to /content/gdrive/MyDrive/models/task2_pretrain.h5\n",
            "Epoch 12/20\n",
            "30/30 [==============================] - 0s 5ms/step - loss: 0.7425 - accuracy: 0.7629 - val_loss: 0.9054 - val_accuracy: 0.7100\n",
            "\n",
            "Epoch 00012: val_accuracy improved from 0.67000 to 0.71000, saving model to /content/gdrive/MyDrive/models/task2_pretrain.h5\n",
            "Epoch 13/20\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6532 - accuracy: 0.7975 - val_loss: 0.8543 - val_accuracy: 0.7600\n",
            "\n",
            "Epoch 00013: val_accuracy improved from 0.71000 to 0.76000, saving model to /content/gdrive/MyDrive/models/task2_pretrain.h5\n",
            "Epoch 14/20\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6118 - accuracy: 0.7978 - val_loss: 0.7771 - val_accuracy: 0.7800\n",
            "\n",
            "Epoch 00014: val_accuracy improved from 0.76000 to 0.78000, saving model to /content/gdrive/MyDrive/models/task2_pretrain.h5\n",
            "Epoch 15/20\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5054 - accuracy: 0.8596 - val_loss: 0.8028 - val_accuracy: 0.7600\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.78000\n",
            "Epoch 16/20\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.4747 - accuracy: 0.8835 - val_loss: 0.6998 - val_accuracy: 0.7900\n",
            "\n",
            "Epoch 00016: val_accuracy improved from 0.78000 to 0.79000, saving model to /content/gdrive/MyDrive/models/task2_pretrain.h5\n",
            "Epoch 17/20\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.4193 - accuracy: 0.8967 - val_loss: 0.5844 - val_accuracy: 0.8200\n",
            "\n",
            "Epoch 00017: val_accuracy improved from 0.79000 to 0.82000, saving model to /content/gdrive/MyDrive/models/task2_pretrain.h5\n",
            "Epoch 18/20\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.4016 - accuracy: 0.8924 - val_loss: 0.5326 - val_accuracy: 0.8300\n",
            "\n",
            "Epoch 00018: val_accuracy improved from 0.82000 to 0.83000, saving model to /content/gdrive/MyDrive/models/task2_pretrain.h5\n",
            "Epoch 19/20\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3691 - accuracy: 0.8951 - val_loss: 0.4810 - val_accuracy: 0.8400\n",
            "\n",
            "Epoch 00019: val_accuracy improved from 0.83000 to 0.84000, saving model to /content/gdrive/MyDrive/models/task2_pretrain.h5\n",
            "Epoch 20/20\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.3560 - accuracy: 0.8875 - val_loss: 0.4948 - val_accuracy: 0.8200\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.84000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHP50EHwez3M",
        "outputId": "d83eefad-614f-4320-8fe5-521bf8c5f537"
      },
      "source": [
        "saved_model = load_model('/content/gdrive/MyDrive/models/task2_pretrain.h5')\n",
        "scores = saved_model.evaluate(\n",
        "  test_images,\n",
        "  to_categorical(test_labels)\n",
        ")\n",
        "\n",
        "print('Test accuracy:', scores[1])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5/5 [==============================] - 0s 11ms/step - loss: 0.7091 - accuracy: 0.8071\n",
            "Test accuracy: 0.8071428537368774\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWzE4TYLaKLS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd160dce-ced8-4610-dd87-5861040be25b"
      },
      "source": [
        "(train_imagesall, train_labels), (test_imagesall, test_labels) = mnist.load_data()\n",
        "\n",
        "# train_imagesall=train_imagesall[:,:,:,np.newaxis]\n",
        "# test_imagesall=test_imagesall[:,:,:,np.newaxis]\n",
        "# train_images=([])\n",
        "# test_images1=([])\n",
        "# for i in range(0,60000):\n",
        "\n",
        "#   #print(a.shape)\n",
        "#   train_images.append(np.concatenate((train_imagesall[i],train_imagesall[i],train_imagesall[i]),axis=2))\n",
        "# for i in range(0,10000):\n",
        "#   test_images1.append(np.concatenate((test_imagesall[i],test_imagesall[i],test_imagesall[i]),axis=2))\n",
        "\n",
        "train_images=train_imagesall.reshape(60000,28,28,1)\n",
        "test_images1=test_imagesall.reshape(10000,28,28,1)\n",
        "\n",
        "val_images = test_images1[0:2000,:,:]\n",
        "val_labels = test_labels[0:2000]\n",
        "test_images = test_images1[2000:,:,:]\n",
        "test_labels = test_labels[2000:]\n",
        "\n",
        "\n",
        "train_images = (train_images / 255) - 0.5\n",
        "test_images = (test_images / 255) - 0.5\n",
        "val_images = (val_images / 255) - 0.5\n",
        "\n",
        "\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience = 8)\n",
        "mc = ModelCheckpoint('/content/gdrive/MyDrive/models/task2_mnist_pretrain.h5', monitor='val_accuracy', mode='max', \n",
        "                     verbose=1, save_best_only=True)\n",
        "\n",
        "# Train the model.\n",
        "start_time = time.time()\n",
        "history=saved_model.fit(\n",
        "  train_images,\n",
        "  to_categorical(train_labels),\n",
        "  validation_data=(val_images, to_categorical(val_labels)),  \n",
        "  epochs=5,\n",
        "  batch_size=32,\n",
        "  shuffle = True,\n",
        "  callbacks=[es,mc],\n",
        ")\n",
        "\n",
        "scores = saved_model.evaluate(\n",
        "  test_images,\n",
        "  to_categorical(test_labels)\n",
        ")\n",
        "\n",
        "print('Test accuracy:', scores[1])\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2627 - accuracy: 0.9260 - val_loss: 0.1888 - val_accuracy: 0.9345\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.93450, saving model to /content/gdrive/MyDrive/models/task2_mnist_pretrain.h5\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0882 - accuracy: 0.9732 - val_loss: 0.0953 - val_accuracy: 0.9725\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.93450 to 0.97250, saving model to /content/gdrive/MyDrive/models/task2_mnist_pretrain.h5\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0642 - accuracy: 0.9805 - val_loss: 0.0718 - val_accuracy: 0.9760\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.97250 to 0.97600, saving model to /content/gdrive/MyDrive/models/task2_mnist_pretrain.h5\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0508 - accuracy: 0.9839 - val_loss: 0.0565 - val_accuracy: 0.9810\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.97600 to 0.98100, saving model to /content/gdrive/MyDrive/models/task2_mnist_pretrain.h5\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0428 - accuracy: 0.9874 - val_loss: 0.0544 - val_accuracy: 0.9825\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.98100 to 0.98250, saving model to /content/gdrive/MyDrive/models/task2_mnist_pretrain.h5\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 0.0353 - accuracy: 0.9887\n",
            "Test accuracy: 0.9887499809265137\n",
            "--- 29.145846366882324 seconds ---\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujpEbmDR3PfD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}